{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02ea6ea566df7354a9a88bcce4c6e714",
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement the CART (Classification and Regression Tree) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere, and a artificial \"spiral\" dataset. The first dataset will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere).\n",
    "\n",
    "**You will be using a regression tree with squared loss impurity to do classification. This is possible here because all classification problems can be framed as regression problems. You could also have used a classification tree with Gini impurity equivalently.**\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Implementing Regression Trees</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ad00e0439033b41698216da17731123",
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a36ccf5bfced96b28d8966f36446473",
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9495e5cadae1976dadd4b1ad0e63367c",
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in spiral dataset: 150\n",
      "Number of testing points in spiral dataset: 150\n",
      "Number of features in spiral dataset: 2\n"
     ]
    }
   ],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "print(f'Number of training points in spiral dataset: {xTrSpiral.shape[0]}')\n",
    "print(f'Number of testing points in spiral dataset: {xTeSpiral.shape[0]}')\n",
    "print(f'Number of features in spiral dataset: {xTrSpiral.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d9205e7c90b804b83fed62414fed1e",
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each data point $[\\mathbf{x}]_i$ in the spiral data has 2 dimensions and the label $y_i$ is either $-1$ or $+1$. We can plot `xTrSpiral` to see the points, colored by the label they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb3182ab160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABdCElEQVR4nO2dd3RU1deGnz09PYHQpNsQFLGgqIiKvSJ29LP33jv23nvFgh1s+FNsCIIFBRHFghTFQlFqEtKn3v39cYfAZJKQkElmJpxnLZfkzi3vTWb2nLvPPu8WVcVgMBgMbQNHsgUYDAaDIXGYoG4wGAxtCBPUDQaDoQ1hgrrBYDC0IUxQNxgMhjaEKxkXLSws1F69eiXj0gaDwZC2/PDDD6tUtUND+yQlqPfq1YuZM2cm49IGg8GQtojIwvXtY9IvBoPB0IYwQd1gMBjaECaoGwwGQxvCBHWDwWBoQ5igbjAYDG0IE9QNaYVVNQ5rxRCsZVthrToMDf6QbEkGQ0phgrohqaiGsCrfwCoagVV8Fhr4pv59/ROh7FawlgMWhOejxaej4UWtpDWA+qeg/smoVrfKNQ2GppKQOnURyQeeB7YBFDhdVacl4tyGlkMj/0HgS5BM8O6DOLJbX8PqCyD4HUSDpAZnoLnX48g8Ln7fiqeA2sE0jFaPRXKublmdwZ/RkjMAa+3GgmcRz04tel2DoakkaqT+KPCpqm4FDADmJui8hhbCqhyLrjwALbsbLb0ZXbkHGprdqho0NAcCawO6TTWU349qOP4Aq7iOs4QgsqylJAKgaqGrzwctA62o+U9Lzkc11KLXNhiaSrODuojkAXsALwCoalBVVzf3vIaWQyNFUH4nEAD8QJUdpFZfTqs2TQnPB5E6BAbAKo3f7t2LuIdLyUS8+7SEurWEfwetrOOFCIR+bdlrGwxNJBEj9d7ASmC0iMwSkedFJKv2TiJytojMFJGZK1euTMBlDRtMcBpIHZm3yH/1jIZbCFcfqOtLRLzgyIvfnHMJODuDZAFOO23k3gl8B7asTskAtep4wQLxtey1DYYmkoig7gJ2AJ5W1e2BSuDa2jup6ihVHaiqAzt0aNCPxtDSOArqf00yWk2GuPuBdxdg3WtmQM7VSB1fOuJohxR+iuTdjWRfiuQ/ixSMQsTZsjpdPcG1GbDudRzg6ASuvi16bYOhqSRionQJsERVv4v+/A51BHVDCuEZBJIbzWWvGYF6wXcg4shsVSmS/wRa9Q74PwDJQbJORby71b+/eFp+ZF7XdQueRVdfsjbd4toKKXgMqSt9ZDAkkWYHdVVdJiKLRaSPqs4H9gHmNF+aoaUQcUH7N9DV10Loe8AFGYcjuTcmQYsbyToeso5v9Ws3BXF2RNqPQa1iUEWc7ZMtyWCok0RZ714EvC4iHuAv4LQEndfQQoizK9L+1Wj1hhMRs2ShMYijXbIlGAwNkpCgrqo/AQMTcS5D6yLiTrYEg8GQQMzwzGAwGNoQJqgbDAZDG8IEdYPBYGhDmKBuMBgMbYikNJ42NA/VCASnQ2QpeHZEXL2TLclgMKQIJqinGWqVoEUjwFoRXbpuoZkjkJzrzUIYg8Fg0i/phpbfB5HFUYOpaiAAVW9BaGaypRkMhhTAjNTTDf8koLYtrR/1TzLe3hsZf89exJi7xrFwzhIG7LU1I64dTrvODfj6GDYKTFBPNyQLtLYtrQscuUmR05apLKti5eIiOvfuiC/Tm2w5MSz46W8uG3IjgeogaimL5i7hize/4fnfHia3XU6y5RmSiEm/pBtZpxHragjgQjKOSIaaNomq8ty1r3Fs5zO5ZLeRHN3xDMY99lGyZcUweuQYAlUB1LKti8OhCJVl1Xw8alKSlRmSjRmppxmSeTJqrYaqF0FD4OyM5N6NODdJtrQ2w6RXv+KDJz8l6A8R9NudjV68fgyb9u/JdkO3aZFrBgMhpoyZyk9TZtOzbzcOPGNv8jvEe8qv4e/Zi+Ks6IPVQX7/4a8W0WdIH0xQTyCqCsHpqP9TcGQjGccgrl4JvYaIIDmXoNkX2Na5km2qXhLM/574BH9lIGZboCrAh6MmtkhQDwZCXLLbSJb8/h/+ygAen5s373+fp76/ly6bdqrzmC133IxVS4pjOlV5Mzz023XLhOszpBcm/ZJAtPwudPW5UD0GKkejq4ahgS9b5FoiLsSRYwJ6CxAJ1dEfFQj5W6Yf6eQ3ptYEdICgP0RVaRXPX/d6vcecdscIfNlenC67cYfb5yavQy4HndnCrf0MKY8J6glCwwuhauw6TZTDgB8tHdm6fT8NzeaA04bizfTEbPNleTng1KEtcr2fJv8a92RgWcrsqfPqPaZnv+48O+sBDj5rX/rv0Zfjrz2CZ2bdT1Zu6zY5MaQeJv2SKEKzQJxQO35bq0FLQIwPd7ow7PwD+e3b+Uz7YCZur4tgIMzBZ+7DrsNaxl26R9+ueHzumvz9Gjr3brjtY5dNO3Hxk2e2iCZD+mKCeqJwdq3vBZDsVpViaB5Ol5Mbxl7O0r+Ws+T3/+jdvweFXVuu09FBZ+7L2w+MJxQM11SzeDM8nHZ78rtB/btgKY+e9xyzp84lKz+LEdcM58hLDjFpvxTGBPVE4R4Izp4QXgBER1ySAZmn2X01Dc3Gsiymf/gDX709jZx22Rx6zn707Ne9xa7XZdNO9U5UJpKCjnk8MeNunr/2dX77Zh6de3fitDtGtFilTWOpKq/m4t1GUl5UgaqyenkpL90wFsuyOObyYUnVZqgfSUa+d+DAgTpzZttb1q5WOVr+CAQ+Bcm0A3rm8WZUkyDuO/UJvn53Ov7KAA6nA7fXxY1vXcGgg3do1PGVZVVUlVVT2LWd+Zs0gk9HT+HJi1+Iy/fndcjlneUvoKosmvcv3gwPnXt1TJLKjQsR+UFVG8wDmpF6AhFHDpJ3I9D6DZzbOn//upCv3p5GoDoIgBWxCFQFeeScZ3lj0TMNBulgIMRDZz7NV+9MQxwO8gpzuP71S9hm976tJT8tKVm2Oi7PD1BRUsmCWX9z0/B7KS+uwIpY9NqmB7d/cI2xKUgBTPWLoVGULF/NrUc9wMEZxzO84BSeu/pVwvWU/rUEc79bAHUE7tUrSqksrWrw2FFXvsLX474jFAgTrA6ycnER1x18F2VF5S0lt02w/T7b4PbG9rAVga122YJr9r+NlYuL8FcGCPpD/PnT39x2zINJUmpYFxPUDevFsiwu2+Mmpo2fSSgQprK0ivef/JTHL3y+1TR03bwzDkd8UPdkeMjI8TV47KcvTiYYHeGvQS3l63enJ1RjW6PPTpuzz//tji/LizgEb6aXrPws9jtpT8KhSMy+kbDF7zP/pGRFbV8iQ2tj0i/NQEO/Q3AaONqDb19EGg4u6cqvX82leFkJkfDaD3KgOsjEV77i3IdOJSOr5e972z370XWLLiycs4RQwE4J+LK8/N8NR+N0Ous9TlXrfKJYk74x1I+IcOkz53DAqUOZ9flsCjrlseexuzL7m/n1HYEVsVpVoyGehAV1EXECM4F/VfXQRJ03VbHK7oGqNwALxA1ld0L7NxFXj2RLSzgly1fX/YLA378sZN53C/Bledn9qEEt5hAoIjww5RZev+Ndvnp7Gll5mRx71eHsfcLu6z1u4AHbMXPCT0TC1jrbabG687aEiNBv1z7027VPzbbthm4dv59D6NG3K+27mJx6sklY9YuIXA4MBHLXF9TTvfpFQ3Ps7kP419nqAM+uONqNTpasFqNoaQknbXZB3DL57IIsAlUBQHA6HYhDuGfCDTEBIBUoXlbC5XveTPGyEkSEUCDMuQ+dwrDzDki2tLTl5y9/4+bh96GqqKXkdcjlvok3tUoJ6MZMY6pfEhLURaQb8DJwJ3B5mw/qFc+iFY8AkVqvuHB0npMERS3P2w9+wEs3vUkkHMHpciIiRMJhwsHY30HnXh145c8nU65k0LIsZk+dR+nKMvrv0bdBB0RD4wgFQ8yZ9ju+TC9bDtxsg/7m1RXVvP3geKaO+468Drkcd/VwBu4/oAXUtg1as6TxEeBqoN5nbxE5GzgboEePNE9ROPJBPOv4vERpwytHj7liGDsfvAPTPphJRrYPt9fFM1e8QjgY+zsoWraa4mWrU+4x3OFwsO0e/ZIto03h9rgZsGd8KqaxRCIRLt/zZhbOXVLzFDh3+h9cNuoc9jlhSKJkbnQ0u/pFRA4FVqjqDw3tp6qjVHWgqg7s0KFhT4uUx3cw8d+HGdEGFm2Xnn27MeKa4Rx+wYFsslnnundSyFxPNYrBAPDjxF/494+lMWm9QFWAUVe9mkRV6U8iShoHA8NE5B9gLLC3iLyWgPOmLOLIQdqNAVd/QOwWc9lnIlnnJFtaq7Htnv3I75CLw7n2LeTJ8DB0xGAysmt3ZjIY4lk87z/CwfjKpOKlJUQitVObhsbS7KCuqtepajdV7QWMACar6onNVpbiiHtLHIXvIp3mIB1/xJF9ESIbT9m/w+Hgoa9uY+ABA3A4HXgzvRx85j5c8szZyZZmSBM2274XTk98Brhz744NlqkaGsbUqTcTu5Jz46Rwk3bc+eH1qGrKTYwaUp9t9+hH30FbMGfa7wSqAohD8PjcXPj4GcmWltYYQ6+NgLLicj55/nMWzPqbfrv14YBTh5KZY1IkhuQTDoWZ+MqXfPn2NNp1yefIiw9h8+17J1tWytJqJY1NxQT11qNoaQnnbn8VVeXVBKuDeDM9FHTK5+kf7iM7PyvZ8gwGQxNoTFDfeJLAGylv3DWO8pKKGu+TQFWQ4qUlfPD0hCQrMxg2jDWWv//9uSzZUlISk1OPohqE6g/R4Ffg7G77oDs3SbasZvPLF78RqWW+FPSHmDXpV0647sgkqTIYNoy/Zy/ixmH3ULqiDFWl65ZduGP8dXTo1nKdqdINM1IHVMNo8Ulo+a3g/xgqX0RXHYyG5iZbWrPp1meTuElMp9tJr21armOQwdASRMIRrtnvNpb/sxJ/VYBAdZB/Zi/m5uH3JltaSmGCOkDgcwjPX2eFaAi0Ci27M6myEsGJNx6NJ2NtOz0Ru8LgyEsPSaIqg6Hp/PbtfPy1nDWtiMXCOUtYsWhlklSlHiaoAxr8EbSORgvh31pfTILZbEAvHvziVnbcfwCFXdux67CBPD7tLrr0NsZL6YqqkowCh2RjRay6+qREfYiM5e8aTE4dEFdvlAyglpdLG8ipA/QZuBn3fHpDsmUY1oOqhVaOgsqXQSvBOwTJvQlxdoq+HkDL7obqd4Ew6hmM5N1Z83pbZ+vBfWJWMIMd0Dv2KKRzb9MjdQ1mpA7gOxQc2cR+x/mQ7CuTpWi9VJZW8vaDH3DbMQ8y5u5xpjVbElC1sKrexlp1JNaq4ViVY1Dd8BGjVjwCFU+DFgF+CExGi45F1fZG0dKR0YAeACIQ/AYtPrFZ10wn3B43d350PTntssnMzSAjx0dht/bc9v41ZvHbOpg69SgaWY5WPAqBb8DZBcm+CPEOTrasOikrLue8Ha6mdGUZgeogHp+bzNxMnv3pftP4t4VQDQMWImvnJ6yy2+wgWzMXkwEZB+PIu3sDzh9BV+xQh/NnFpL3IHh2RlfsAgTjXy8YhXh2avI105VwKMzc6X/g8rjos9NmOBwbz9i0Na130x5xdkLy7kq2jEbxv8c/oWR5aU1bt6A/hBWpYMzd73HBo6cnWV3roVYxWvUWhP9EPLtAxqGIeBN8jQq07EbwfwZYqGdXJO8eEBdUvY09al5DNVSPR7MvQZz1uFjWe6EAaB3t9TQC1grQCqCe0ahV3LRrpTkut4v+Q/omW0bKsvF8xbUhfpoyuyagryEcivDLl22zQUddaHgxuvIAqHgS/O+j5bdFUxWB2P2sMjTwFRr6bYMmF3X1peCfCISwUx7T0OJT0PBiu41hbcQLkUVNvo44MsHZqy4F4BkEjk52L9y4l8OwnlG6qmJVjsFaeTDWyn2xyh+312UY2iQmqKchvbfpgdMVayQmDqFnv25JUtT6aMWDoOXUjJS1GsL/QPX4mn2sqrfRFYPR1ZeixSegRcNRq6Tx14isguB0YlMeEbCW2tVSGqrjoCC4Nt+QW4o+AWQBXuyPpg8yT0ZcvRERJP8h+3XJiO7jhZzrEEe7hu+j/F6ouAciC+wvnMpRaMl5G6RRVdHwP2hkxQYdb2h5TPolDTn68sOY+OqX+CusqEMieDM8HH/9xrFCVFUh8C1Qe4KwGg1+i2QejYYXQdltQMBObQCEF6BltyL5jzTyQuVAXS6cDoQwmn0+VDxDTdWUZEDmKesNsvUhngFQOAn8H6FWKeLbC3H3X+f1HaDDF+CfAOoH796Iq+EvcrUqoOp1YtNEAQh+j4b+QNxbNFqfhn5BSy4EqxSIoJ4dkPzHEYdpDZhKmKCehnTZtBOPT7+b0SPH8MePf9Frmx6cdscIem+T5m0Cscv28H8SDThbg2/fmMlJVUVLrwAtq+NoL7g2tf8ZmAjUTreEwD+p8WKcPe2qKKvW5KWGwb09Du8Q1N0frXoTsJDMY8CzZ+PPXwfibA9ZJ9eXPbcDaOaxjT+htQrEGf+rEJc9am9kUFetRotPjeb2owR/QEuvRgqebbweQ4tjgnqa0rNvN24Zd1WyZSQUtUrRoqPsQKRVKJlQ+Qy0fxORqFVw6Bfwf078KB0QL5IxIvqDmzonFqXxb3kRB+Q/ipacaQdFUVAL8u5BHNnRS+6OeHdvym22Ls5NqDPLqkFwN6G/aOAr6vySDHyNWlX2nECKEw6FmTruO36b9js9+3Zjn//bvU126TJB3ZAyaOVzEFmKPSkJUAXhf9DKsUh2tP9r6EcgvgUaAO3eRpyF9r99B0H5g7V28ILviCZpEs9A6PA1BCbZOXTv3muvkQaIeNCckVB2K/bcgGWniTJOaFqFTl3zBzWkfp180B/k0iE3snj+f/gr/PiyvLx2+9s8NfPeJpcBW5ZFOBTB461jojwFMBOlhtQh8CVrA/oa/BCYsvZH5yawTjqmBinA4V7bXEGcHZCCJ0EKohOLHvDuheRe02RZ4shBMo5AMo9Nq4C+BkfmUUj71yHjGPAdjuQ/geRc3bSTeIfY5ZWxZwb3tjVPLanMhJe+YNHcJfgr/AD4KwOsXlnGq7e93ehzqCqv3/EOwwtO4bCs/+OMrS9lzvTfW0ryBrNRjtRVFUKz7Ppf9w6I0ywxTgkcmwDza28E1zqOkt6h4MiHSIC1I/YMyL4w7nTi3R06fguRheDI3+AJzLaAuPsjef3Xv2N9xzvy7FRU6WXYY0ELHIV2RU4aMPPTnwjUMgOLhCL8OPGXRp/jf49/zNh7/oe/yp50XjT3X67d/3ZGz3+M9l1SZ9HfRhfU1VqNFp8IkSWAgIbQ7ItxZJuGyYlEQ/NR/0eAE8k4DFkzgdkAkn0uWjwN8K+z0Ytknbb2R/FAu3fQivvtkb3kQdY5ODLrTquIONdOnhqahfiGgncaBH8AyQb3gLRZnt9l80643E7CtXoLdOzZodHnePuB8TUBfQ3hUITPX/+KY688PCE6E8FGl37Rsrsg/Fe0zrgSCELFE23COz1VsCpfR4uOgcpRUPksumo41jr14/Uhnu2RgqfB1QfIAFd/pOBFpFbdtzjb48i7B0fHaTg6fFpvQDckHpEMe3LYs13aBHSAwy84EHetHLg308OJNx7d6HNUlsU7uYaCIUpX1lWJlTw2uqBul7rVnmgLov6JyVDT5lCrHMrvwR5tW9i/az+U3RS32rMuxDsYR+F4HJ1/xlH4LuLZsYUVG5KNagQNTEf9n9t19S1Al96deOir29h2z35k5maw2YBe3DLuagbs2fgKoJ0O3B6nKzZk+jK87HpYg1YsrU6z0y8i0h14BeiEXfM0SlUfbe55Ww4vUFlrmys6mZYerPq3iEXz/qNH364UbpJieeLQb/ZEZl0BPPwnuPu1viZDq6AagcjfILmNnqfS8CI7Harl2OnQMJp3P46MAxKub/PtevPglFs3+PjzHj6Vud/9TnlxJVbYTuMccNpQth68VaIkJoRE5NTDwBWq+qOI5AA/iMhEVU1NI5LM46HyBWLytjiQjMOSpajRWJbFo+c9x8RXvsTjcxP0hzjwjL256PEzWuxRuC53wgZxdq5n+XwIHI3PXxrSCw18h5ZeYq901Qjq2QnJf2y9lTG6+nK7YGHdssjSq1DvIMSR36Kam0r7LgW8/PvjzPhkFquWFLPtnv3otXXqtYVsdvpFVZeq6o/Rf5cDc4GuzT1vSyHZF0DGcOwRuw8cHZCCp5vuqpcEPn/taya//jWhQIjK0ipCgRATX/6CKWO/Sfi1VP1Ypdejy7dDl2+LVXSCbWK1HsTVCzw7Yv9+1+AF7z6I0wT1tohapejqs223SK3CtiGYgZbdtJ7jyiE8h7g6d3FCYGqL6W0OLreL3YbtxLDzD0jJgA4JzqmLSC9ge+C7Ol47W0RmisjMlSuT109QxIUj7zak43SkwwSkw9et4pv+69dzufOER7hp+L189c60DXIM/OSFz+Nm3/2VAT594fNEyQRsIystOgmq/0fNgpXQj2jx/0VH7g0jBU9B5gi7RtzRHrJORfLvT6hGQwrhnwRa+0kxCP4JdkqmXpzUayecYAvljYmElTSKSDbwLnCparwxh6qOAkaB3SQjUdfdUMSRBWS1yrU+HDWRZy5/qaZOdtbnv/LDpF+47JlzmnSe2q281uB0J64y1ar+CEqvIa4ZA5ad9wxOh/UsixfJQHJHQu7IhOlqCVSVlYtXkZWXSVZe67wX2iZh4i0EwB6B1/9RF0cm6t0ruuhs3febC7x7JFThxkRCooGIuLED+uuqOi4R52wrhIIhnrvq1ZiFD/7KAJNe+ZIR1wxvUgPoYecfwPzvF+CvXDta92V5Oezc/ROiVa0KKL2O+IC+DtbqhFwrGVRX+pn48pfMnjqXrPwspo+fSVlxBWpZDB6+M1e+eD7ejOSMEFWVf2Yvoqrcz5YDN8XtSc0l6HXi3Ru4o9ZGF3gGI+vx2pG8e+28evBbQMDZGcl/NOHNTjYmElH9IsALwFxVTcryMrUqIPQzOArA1Tel6mdXLSnGsuK9MVweF3/+9E+TgvqQo3bhz5/+4Z2HxuPyuAgHwxxz5TB2HZagkqrg97bhVX2DKw2Dd7fEXKuVqSqv5oKdrmHlkqK4lYUA377/PU9f/hKXPm0/PQWqA1RX+MkrzG3x91PR0hKuPeAOlv29HIfTgcPh4KZ3rmD7vdeuAC0rKidQHaSwa7uUen+DbcmgefdB6bV2PhwLnN1sf/j1HevIRtqNQq3Vtie+o3PK3V+60ewepSKyO/A18CtrZzyuV9WP6zsmkT1KrerxUDoy6r4XAWdvpN1oxJEay3b9VQGO7nA6gerYQOLJ8PDU9/fQs1/TJ1sqSytZ+vcKumzaiazcxLnjafBHtOSM6KKs2ngg9yYcTbF9bWGCgRAvXPc6E0ZPIRKOMOSoXTj/kdPIzo9Ppbzz0HheunFs3N9hXdxeN/9b/TJPXvwiE1/5ElA6dC/k2lcvpu+gxvuON5Urht7M7KnzsCJrv/x9WV7e/O85rIjFncc/zM9TfkMcQseeHbjp7StS0mZZrSrbfmM9gyu7B4AJ3BtCY3qUJqL6Zaqqiqpuq6rbRf+rN6AnEo0shdLrAb/t86zVEP4dLW141r018WV6Oe6a4fgy1z5OejM87LBP/w0K6ABZeVlsvl3vhAZ0ANzbgaOQ2LeFE5x9kA6TUiqgA9x/2pN8+OxEKkur8FcGmDL2G67Z//Y6J6FnTf61wYAOEAlHeOG61/j8ta8IBUKEAmH+W7CMa/a/jbLi8ha5h8qyKuZ8Oz8moAM4HA6+//Qn7j7xUX6a8huhYJigP8SS+f9x5d63EAo25JqYHMSRiXgHI+5+dQZtq3o81ord0eV9sFbui6ZohUu6k94rSutseBCGwOcbVF3SUpx449GcfvcJbLJ5ZzbZrBOn3HYcN797ZbJlxSHiQNq9Cp6dsSsTXOA7EGk/JuVKPsuKyvnmvRkE1wnU4WCYRXOX8OfP/8Tt37Nfd1yeuroY2ThdDnbcfwAfPzc5LvhrRPn6nekJ014btep4rwoEq4P8OOlXwsHYiqNwMMyPk35tMT0tgQam2k/UVrQNXmQRWnIBGpqXXGFtkPQO6uKi7pKo1Lqt5697neeveY2S5aspWV7KpNe+ipnsTCXE2RlHu1eQTj8jnX7Gkf9wSlqrrl5ZhtMdH6QdTgcly1bHbR9+0UF4fJ64EaQvy0tGjo9OvTpyxfPnEQrEj+YjkQjVFf647bUJBUOMuXscp/e7lHN3uIoJo6esd3Ax9p736txHVem/R9+6D1JivszSAa0cReyCP4AAWvVKMuS0adLbpdG7P3B3rY0eyDg0ZXJ2s6fO5YMnPyXoD4HffmRePPdfXrjudS55OnWdIRu9gjRJdN28Mx6vu8Yfew3hYJitovnvcCjMjI9nsWLRKrYZshWPT7+bF69/g3kzFtB9q0049qrDCVQFyCvMZevBfXA47NH6zAk/x6RDRIRdDrU9aGZ8MotXb3ubov+K2enA7Tn1tuMo6JQPwC1HPsDPU2bXjPSfuPgFlvzxH2fc9X913sPyhSsZ98hHWLVG6iLCHeOvpXOvjvTo25W/f1kUE/gjkQg77LvhNrpJIbKqjo0WmAbWCSetg7o420PB0+jqy4BgTXWG5KROTv2rd6cTqLVgKBQM8/W736V0UE91nC4nV798Ibcf+yBWxMKKKE63k3MfPJmcgmxKlq/m4l1HUlpURiQUQRzC0BG7c/O7Vzb4hX/Zs+dw6ZAbKSuqAFUi4Qin3XE83bbchK/emcZ9pz5RUz0z4aUpzPhkFqPnPcryf1bEBHSwS1ffe/RjThh5FBlZvrhrzZn2u/204Y/Nj7u9LjbZvAsAN755OVcMvQV/hZ9wKEwoELYnUe97nxNGHhUzV5PS+PaDykXElstmIL6GPV7UKkcrnrSN+BwFSNY5iG+/FpWa7qR1UAfb1Y+O02w7XUd+yi1Fz8zJwOFyEqnl4+zNSt6HUYMz0eqPwZGJZBzZKK/zVGTQwTvw4txH+fKtaYSDYQYfsTM9trIdKp696lVWLikiEl77e//izW8Yevzu7LBP/aPcwq7tefmPx/lp8mxWryhjwNCta0zTXrj+jZhyyEgoQsXqSr5+ZzpZeZl2gK7Vo1qcDoqXltA1GqTXpWOPQurMzoiQU2BX8HTbchPeWPg095z8ON+M+w5VpXRlOe8+/CGzv5nHQ1/c1uDvKBKJ8MNnv7Ds7xX023VLNt++d4P7txSSdRYa+NzuY6BBwAOeAVHLjrpRjaBFx9lNTghBZDG6+ko0d2TKTdqnEmkf1CHaCKGRXdFbmwNOHco7D42PCereTC9HXnxQUvRY5Q9B1cu28RJOtPIVyH8I8e2bFD3NpWP3Qo65It6MbcbHP8YEdLBHztPGz2wwqAM4nU523G9A3PZVS4ritvkr/Pz35zIOOmPvuAlNAIcIHXvU3QKv365b0m2LLvzz2+KaY72ZXg47b/+YRVChYJhv3/+e0DrnD/pD/PHDXyyY9Xe9gbqsuJxLBt9A0X/FRMIRRBwMOWoQV790YaunJ8WRDe3ftxtYR/4C19bgGdSwjsBXYK3bsxagGsofgjQL6oHqAJPfmMrc7/5g8+16s+9Je5CZ0zLOsKk1o9gG6bJpJ25+50radynA7XPjzfBw+AUHcMQlh7S6Fo0shcoX7dJPlDVe51o6cj0eHelHdn7d5Z5//PDXBp9z0wG94rb5sn1stfPmdOzRgYPP2hdf9AlMRPBmejn3oVPqXR0qIjww+WYOPG0oeYW5dOxRyKm3HcdZ954Ys1/pyrI6g5/D4eC/P5fVq3f0DWNY9tdyqsv9BKtDBKoCTB33HTM+/rEJd504RJyIbyiSdQbi3WX9XyyRRfU4fpY0yoMoVagsq+Lc7a/iqUtH88nznzPq6lc5c5vLKF3VMs012sRIPdXZ6cDteWPxM6xeUUp2fhYeX5ImIUM/g7ijj7/roNVgLUfFZ4+OxAfePZEU8JhfOGcxj13wPHOm/U5eYQ4n3ng0h5y933oDwrFXD+ex85+LKxf844c/+ePHv9hih6annC58/AyuHHoz4WCEcMjObW+546YMPHA7AM5/5DR22HdbPn/tK7yZXg45Zz/67bJlg+fMysvikqfPbnB+pbBrOzxeV9zcTDgUps9Om9dzFHz7/sy49m3+ygBT/zeDQYfYE7+qytzv/mDutN/p3Lsjgw7ZAVcCvYSahbu/vUK1dorK2WO99gOpxAdPfsqKRavsYgkgUBWgJBzhzfv+x9n3nZzw66XPbybNcTgctOvc8CrX1StLqS7307l3x5Z5PHZuAhpvWQCKBmZA2U2AA0Ts/7d7GXFvk3gdjaSsuJxLd7+RytJKVKHovxKevfIVxCEcclbDk2WHnLUvj1/wPForIkTCEWZO+HmDgnqfgZvx3K8P8eEzn7F84UoGHbIjex23G06nXVopIux62MCEd8Jxupxc/NRZPHDGU4T8ISxL8WV5OeSsfenUQI/N7PwsipeWxGxzuZ3kF+YCtj//nSMeYcYnPxIORXB7XRR0zOOxaXeRF90nqbi3B8+uEJwWfbp0Ah4k95YkC2saMz/7uSagryEcDPPDZ7/AfYm/ngnqKUBVeTV3nfAIP076BYfTQW77HG548/L1jvKajKs/uLaE8FzWViFkQMYR0YAeLQ+MxkFdfTEUfp608tDJb0wlFAzFTCb6KwO8cdd7MUFdVZky9hvefWg81RV+9j5hd4696nCy8zPtKpZ1cHvc5LTb8Lr7zr06cuY9J65/xwSz13GD6dG3Gx89Nwl/hZ+hx+/Ojvtt2+Axx119OI9f8HyMXbPT7eKgM/cBYPr4H5jx6ayaNRPhYJiQP8ToG8Zy6TPJr8wSEch/EvyfoP7P7N4HWcfH9axNdbpt2SXOBkJE2GTzllnQl5ZB3a7ZtewJ0jbAw2c/w4+TfiEUsPOEK6uKuPaAOxi75NmETqaICLQbjVY8Av6PAR9knQTO7uD/IP4xN7LSnqhybpIwDU2h6N/iOs23ymrlIsfe8x5v3DWuJjiNufs9Zk2ezZGXHMKYe/4Xk7ZwupzseeyuLSu8hdh0255c9PgZjd5/v5P3pKyonNfueIfqsmo69+7EZaPOYZPN7GDy7fsz4uv8QxGmfzgTSH5Qh2gRRMahSMahyZaywRxzxTAmvzE1ZsGhJ8PDCdcf2SLXS6uJUtUQVtmd6PIB6PKtsYpObFQ3nlQmHAozddx3NQG9BlWmj0+M6dm6iCMbR+4NODp+i6PjZBxZpyGOXOq2ZtSk9m7dYb9tayYe1yAOYds91/Y5DQVDvHH3ezEfmKA/xO8z/2SH/Qdw9BWHkpHtQ0TYfIfePPjlreQUpN4K2ZZARDj68sMYt2o04yte4+U/Hme7oWvTaQWdC3DVsSo3t31Oa8ps83TbchMe+vI2tt9nG3LbZ7PN7ltx72c3blAKsDGk1Uhdy+6E6nHUpAlCM9HiEdBhSsqvgKwPy9K4FYVgP43UzsO1GO4dQfKiecs1j4ge8OyWVLfL7YZuw67DBjLtg5kEqoJ4Mz14Mjxc+Nja0WpZUUVNE+B1EYewdMEyTr11BKfcchxWxMLpahtPdk3F4XDUOTl/8Fn78L/HP46ZTPVmehlx7RGtKa/NsWLRSj54agL//bmcnQ7cjn1O3IMtdtiU+ybe3CrXT5ugrhqC6neBdSsALLsnYuBr8O2TLGnNwuN1039IX379em5Mzs2KWAw6ZIdW0SDihHavoaVXQugXQMA7FMmrbcHQuogI1712CbOnzuOnKbPp0K09ex67KxnZa58e8jvm4svyxn0BRkIRtthx05rzbKwBvSG69O7E3Z+M5JFzR7FwzhLyCnM45bbj2Pv4hjtbGernr18WcumQGwgFQoSDEb7/dBYfjZrII1PvaLWqovRJv2gAu6467gWwSurYnj5c88pFdO7VgYwcH5m5GXgyPFz10gU1niKtgbi64Wg/Fun4PdLpBxwFj8cZealViVV2N9aK3bFW7odV+SpaZzVNAnWJ0H9IX0666RgOPH3vmIAO9kKhCx4/A2+mB3HYE7q+LC9Dj9+d7n1Stv95yrDN7n15+sf7OOqyQwkFwjx16UvcfMR9FC9L789Usnj68peoLvcTDtpPP/7KAAvn/Ms3781oNQ3NbpKxIWxokwxr1aEQ/r3WVq/dQDpJk3mJwrIs5k7/g4qSCrYZ0jfxXunNRFXR4uMgNIeYypnME3HkXpVMaQDMn/knHz49gcqyKoYeP4Tdj9g5ZUzdUp0HznyKL8Z8U+Nb43Q56dy7Iy/OfQSHI33GfanA4fmnUFVWFbd9+EUHccGjpzf7/I1pkpE26RcAybsfLT4RNAJE84DZl6Z9QAc777n1bn2SLaN+wr9Gv1DXrUaphqpX0ZyLEIk3rGpN+gzcjD4vnJ9UDelIdUU1k1+fSiiwNn0VCUcoXlrCr1/PZcCeWydRXfrRuVcH/vplYcw2X5a3xpOoNUirr2Fx90U6fInk3YzkXIUUfogju/ElXoZmEF5C3d71pHUz6o2dqnJ/nX9WEWHV4iJKVpSmVMOZRKKqaPhvNLwoYec8857/w5uxdlLa6XKSmZPBPifukbBrrI+0Sr8YkodG/kVXHkjsRDUgBUjHb9vMmoGNDVXlxN7ns2JRrN+5OAW324WqkleYy1WjL2CHfRte7JROaHgBWnKuvRYDBVcvpOBZxBnvptlUZk3+lVdve5sVi1axwz79OfnW42qcPptLY9IvJqgbGo1Vdi9UvYHtL+sE3Ej+gynlbx0MhPj5i98AGLDX1ni8dZtpGWwikQg3HHo3Myf8vHajgNPpIBJeOwnuzfTywm8PN2hLkC6oRtCVe0Vb662Jfw5wbYGjcHwSla2fNpdTNyQXybkavHui/o9BMpHMo1Nqyfa8GX9w3YF3Yll2MBKHcPcnN9B3UGraMqcCE0Z/weyptfqEKjEBHew8+6TXvuT/Rh7diupaiNAvdqP6mAV3FoQXouFFiKtHspQlBBPUNyLUKkar3obIQsSzG/gOQKTxI1kRAe8uiHeXFlS5YUQiEW46/F4qVlfGbL9x2D28+d+oGtMtQyyfvji5Uf1yI6EIVWXV690vPQhT7/wQ6W9BnZCJUhE5UETmi8gCEbk2Eec0JBYN/4Ou3B8qnoDqd9DSG9Dik+xFXW2AP3/6J8a4ag1Bf4gFP/6dBEXpgdtb97iudjWoJ8PD4CMGtYKiVsC9HfHjWQFnR3D2an09CabZQV3sGbIngYOAfsDxItKv4aOahmoEq+JJrOWDsJb1xyo5H40sT+Ql2jxafi9oOWsnOqsgNA/8nzV8nCoa/BGtfAH1T0jZLwFvhgeN1GG3YFl4MtLTQqI1OPyCA+P8dbyZHjp0LyQj20dGtg+3z80RFx+UeNfQJCHiRgqeA8kHyQbJAkdHpOCZhK1tKFlRynuPfczrd77Lnz//k5BzNpZEpF92Bhao6l8AIjIWOByYk4BzA6DlD0LV69Q0gAxMQYvnQuHElDDL/+LNb3jl1rcpWb6a7fbahnMePJnOvTomW1YswR+IN+2qQoPfIRl1d2FStWz73cDXQAjEC4520P5txJGY2fxE0aNvNzr16sDi+f/V2C04nA46dC+k19bdk6wudRly1C78PXsxb933Pi63k0jE4qSbjubYqw7nl6/msGpJMVsP7pP097MGZ6HVH4B47L667uat6RDPAOj4DYR+AlzgHoBIYiq8f/t2PtcecAdWJEI4FGHMXeM47prhnHTTMQk5//podvWLiBwNHKiqZ0Z/PgkYpKoX1trvbKJ+nj169Nhx4cKFceeqC9UQunxHaky8ak6YheQ/jHj3apb+5jL5ja956OxnaixiHQ4hp10OLy94PKVWhVqrDo/6qK+LDzKOQrx7gGcg4oh151P/RHT1VcC6K+RckHEUjrzbW1pyk1mxeBW3HvUAf/9q1x337t+dm9+5ko490r9io6WpKq9mxaJVdOrVgYys2IVk4VCY4qUl5HfMS0rXLqviWah4CjsGCOCB3NtxZB7e6lrWh6pyyhYXsfSv2EyCx+fmxbmPNrt6qDHVL622+EhVR6nqQFUd2KFDE25MA8Q2nl2z3YrWmCaXl256M8bz27IUf1WAKWO+idvXsizmzfiDOdN/JxJp3QkZybkMWPfDKkAQqsehpVegK3bDqvog5hgNTCI2oAOEITC5ZcVuIB27F/LkjHt49a8nefWvJ3lyxr0moDeSzJwMem3dPS6gT3hpCkd3PIPT+13GkYWn8/qd77bqYiS1iu15INb01bUAP5TfitZuy5gClJdUsHJxfINyp8vBL18mLHnRIInIXfwLrPt82y26LSGIIxt19rQ7kMdggSf5EzfFy1bHbQtUBVj+z4qYbQvnLuHaA26nsrQKQfBkeLj7k5H1doJPNOLdCwoeR8sfgchSQEBLgOq1WZmykah3EOLsZP/s6Ij9FqllpJZEO97G0L5LautTjUBwOlirwDMIcbZMB5zm8tu383n8whdimoyMvec9NtmsM0NHDG4dEaHfon11a0+CK4QXgju1ylV9WT4cTokfh4pQ0Dm/VTQkYqT+PbCFiPQW29R8BPDBeo5pEpJ3L0gm9kjTYf8/6/SUqCftt+uWcZUCvmxfTCMHVeWGQ++m6N9iqsv9VJVXs3pFKdcffGerjtjFuyeOwveQjl+Drmatd/oaHBCYsnb/jOPsD1QMGUjWeS2stO2ikRXoqv3Q1ReiZTejK/fDqhiVbFl1Mv7pCQSrY4OpvzLAuEc/aj0Rzk1A63Bn1TA4U+8pzON1c+g5++HNjLUKyO+Qx/b7tE6/32YHdVUNAxcCE4C5wFuq+ltzz7su4hmAdJiM5FyJZF+MtB+LI+eyRF5ig7ng0dPIzM2sqbDwZfvYdo++7Lj/gJp9Fs5ZwuoVpdR+avVXBZj//Z+tKTeKg7r/9GJPhq75ydUNKXgRXNHRkKM95FxX78SqYf1o2c32k5JW2r0ACEDFE2h4QbKlxVFd4Y97zwL4K/3xG1sIcW0GnoHAuhU6PsgYhjjyW01HUzj7gZM55sph5LbPxpPhYddhA3lk6u2ttlYiIaUjqvox8HEizlUf4mgHWSe35CU2iJ79uvPS74/x2ctfsGLhSnbcfzsGHbJDjGWpw+mo88OB2suxWxsRJ+o7CPyfEuO6KIB339h9PTsihR+hqsbKNhEEviJ+gUsY/JMhO3VW5wLs839D+HHSLzGLk7wZHvZtRXMqACl4Cq14Eqr/B7gg83gkK3WN/JxOJ6fcchyn3HJcUq6f/HrANkB+hzyOvbL+mfjufTahY/f2LPljKRptXScC2flZNd15WhvJvRXVymi5ogMc+Uj+I3EVMDX7m4CeGMQHcbX+LnBkJUVOQww5ahdmTviZz1//CpfHRSQcof+Qvhx5aes+qYn4kJwrIOeKVr1uoli9spQPn53I378uYts9+rL/qUPjJqQTiTH0aiWW/rWckYfcxYrFqxARCjrlcfv46+jZt1tSdalVDFYFOLubwN0KWGUPQNUrxJToShbS4fOk1v6HgiG+/d/3LJ7/H5tv35udDtquJl3w74KlLJj1D937bMKm2/ZMmsZ0ZPnClZw38GoClQGC/hDeTC8durXjqZn3xnXxagzG0CuF6LJpJ16Y8whLfrcXx/To2y0lgqg42tkLigytguRcimpFtN+uBc4uSN79SQ3oFasruWiX6yj6r4TqSj8ZWT569+/B/ZNvweN103XzLnTdvPmWtBsjL9/8JpWrq2oWxAWqAqxcUsSnL07miItb5oknrZpkpDsiQvc+XenZz4yKN1ZEXDjybkE6zUQ6TkUKJyKe7ZOqaey977F84UqqK/yg9gTpnz//w4TRU9Z/cAqhod+wVl+KteoYrIonUasi2ZKYPXVeTEN5gEBVkFmfz26xa5qgbjC0ABr+C6v0eqyiE7AqnkKt8pjXRbyIoyAlvtynjf+BUCC2bDBQFeTb979PkqKmo4Hv0KLjwf8JhH+GimfQoqOTvkCp65bxTzhuj4te27ScdYUJ6imGqjJvxh98+uJk5s34o822EmvLaGg2WnQEVL8HoZlQ8TRadCSqDVvX2uZpP6EVT6FVY1GrrFX01tWVx+F00LFHYatcPxFo+V3Y8xRrPi8BsJaBf0ISVcEptxwbU7MuDnvh4bDzD2ixa7b5nLpG/kNLb4HgNHBkQ+bpSNYZCTPvSSThUJgbh93L7KlrPVq2HrwVt39wDW6P6eCTLmj5fRATwAMQWYFWjUeyjq3/uLJbomV7AXu9QPkD0H5sizciOeH6I/nt23kxdhdur5sjLj64Ra+bUML/xG/TKjQ0D8k4rNXlrGGrnbfg/s9vYfQNY1gy/z/67rolp995PIVd27fYNdt0UFcNoEXHgFUEWGBFF3oQRLIvSLa8OD56bhK/fj0n5sM1e+o8Pn7+cw4//8AkKrNRjaBVY6H6TcABGccjmcek5BdkUgnNq2NjNVS/gfqG1NkHU0O/RgN69MtAqwE/Wnoj0n5MC4q12/5d99oljLr6VZYvXEmPvt244NHT0svd0tUr3rBOMhF336TIWZe+g7bgvok3tdr12nRQx/95dNXeuhMV1VD5App1fkrkM9dlyphvYgI62LPlk9+YmhpBvWwkVH9CTeApvwsNz0XybkmmrNTDtRmEfojfHp6PrjoI2r2KuPvHvhaYRrxhiEatYVuewcN3ZvDwnVvlWi2B5I5Ei8/E7heggBccXcC3f5KVtT5te4hlraxjoQf2Eu0UbFuVU1D3ApTcdtmtrCQejSyD6g+pCehg/7v6HbvWvann02o0vCRlm240FQ0vxCo+E2vZthD6tZ69InZKoPgsu0Kj9Hb79wrgLATqsLWV3JaS3LZwFIJ3N5A8kA6QdSrS/m1sO6qNi7Yd1D27UOctuvqlRHON2hx56SExkypgd3Fv7RV8dRJZTHyTDQAHRJY0+jSqilX+MLp8EFp0CLpiF6yq9xImMxmoVW6n+YJfY0/WrafiQovtCo3qMeiqw+zA7j0g6ruz7vs1A7LOajnhbQQNzUeLjoTAl7ZRnZbak9RxhnUbB206qIu7D2QeB2RQ4+4o2UjenUlWVjfb792fi544k9z22bg8LnLbZ3Ph46czYK+tKSsqJxJO3tOF4qROX3v8qKPx9sFa/R5UvgT47byxlkPZzWjolwQpTQL+D1n72N8UwqCVaOXziCMLaf8WeHYFPFHztItT2uMkVdCKh6NzEGs+H0GwytCqt5IpK2mk3nA1wThyR6K+Q+1RlKMAfIekrLsbwAGnDmW/k/eksrSKrLxMvnlvBsdtcjaVpZW4PW5OuvkYjr689WfzJfwnWpe3OoJIE4JZ1UvEpnAAgmjVWCRv22ZpTDQa+c/283b2RNz19+fUyLJa1S5NIQyhnwEQV0+k3egNPM9GTGgu8V+ofkjngUIzaPNBHQBXd7S6xK4uqP4Yss9HvK1k8r8BOBwOcgqyWTDrb+495fGaydNQIMzLN79Jh27t2fPY3VpXlLMbiCfe21oyo17360et4nq6VVlglWKV3Wv7uTs7I1nnId7kNEFRVbvuuWpstEFDGPUMjDYmjs/RimdntPJl4rtEATjB2RO0zJ7f0XJi0wIucKfWl1ltKksrGf/MRH7+Yja9tunBERcfTMfuKVTD7u4LgWXEBnYf1J6M3kho84Zeqn674iCynLWjTB+S/xDi27ehQ5POg2c9zYTRU2qcHdfQZ6fNeOK7e1pVi6qFFg2L1gOvyRlnQPY5OLLPX//xge/Q1WdHJ65rfzFk2B3drbJ1zu2zXSN9eyfsHhqLBr5ESy4m9onCB9ln48i+MH5/1WiD7q9qHeMBRy7S/n3E2cFeM7HqsOioPgy47LK7wg9TpvtRJBJBRPjzp3949PznWDDrb9RSRCAStnC5nXgyPDw54x66bblJsuUC0Zx68XHR7kgR7N97PlL4MeJoWxPNxtAL7BVlVgmxgcSPlt+f8kG9sqQyLqADVJZu6KP+hiPigHZvoOWPQmACSLbdfSpj/R3SVS209LJ6UhQecO8IwR+InWD0o+X3Nimoa/BntPxuCP8Ozl5IzjUbNNrX6g+ITxH5ofp9qCOoiwjkPwbBb1H/FxBZBmLZHeozj61J94lzEygcj1Y8Z1fIuLdFss9KiYBesnw1D5zxFDMn/Iw4BCwlEomfaAyHIkQifkbfOJYb37w8CUrjEXcfaD8OrRwFoQXg3QXJOr3NBfTG0uaDuob/jtaq1yLyX+uLaSJ7jRjM9xN+imlS4PG52WtEK6deoogjF8m7EbixaQdGFoFVWccLPsi9CSL/QnBqHcfV3+pW1bJ7fDryEfGg4T/R4pOpCcbh2WjJWdDuVXB2BP9HqBVAfPsj6+trKWvaJlp1bK/nEBHwDl5vWk+cmyB5Nzd8/VZGVblq31tZMn+pbT61nvl4tZR53/3ROuIaibg2RfJa9+k1VWnzQV3c/VHJjA/srvonvlKF3Y8cxDfvf883475DHIKI0Kt/D467eniypTUNRy51Rgpx2BOQjoJ6/kZ1L49X/yS07EbbBx4HmnUaRFYQX0oYQMvuhPA87AAdQSufRXOuwJF1Sr1yJfM4tHo8MZ7nZEDmSeu707Tk9x/+YvnCVU2qrurZL7l9AAz10+aDOt69wLWVvYRYq7Fv2Y3ktt6y3Q3F4XBw3asX8/c1i/h95p9023KTaKPr5q+EjYQjvPPQeD5+/nNUlYNO35tjrhyGy534t4Q42qHePaM55zVPHS5wdgVXf3BZdh/U0HzsQOoEPEhu/BOBhhegqy8nJuBWjgZHJ+K/OBTCv9baHoHyB9CMw+r1MBf3tmjuLVB+Z3Ri2ILMExuVakpHyorKcTga954SAU+Gl1NvH9HCqtIf25xvAUvm/8fmO/Sm9zY9WuW6bX6iFLDtN6vfRwNTwLkJknkS4tq4O7jce/LjfD1uek1ljTfaIHfkmJZp6K1ajZbdYeeliYB3LyT3DsTZPvp6AK0aB4HPwdkVyTrZbjpcC6vsLqh6lbgALu2iK4UD62x0s2aEHrtvNpL3QFy+XrUaNFCTA1cN2U2iHYWIo3EVPulIVXk1x3Y+k0B1HYumBHwZXhSlXZcCNu3fgxNvPIbNt2/82oREoOEFEF4I7r723ESKE/QHue6gO/l95p+ICFbEYrfDd+La1y6O6V/cVMxEaRQRD2Qeg2S2zZEWwIKf/ub9Jz5l9YpS9jx2N4YeP7je7uVFS0v48u1phAJrFxMFqm3/7JVLiujQLfEOciIZSN6daO4d0Z+l1uteJOt4yDq+4RPVZ/EgbnD2sPPwWgVkgCPfzrvX3l8tO8++5ke1jbPwfwIo6uxlV0e5+4CrdUZXySQzJ4PLXziPB09/GnGs/ducevsI/JUBOvXswJCjBuHN8La6NtUguvoi2xtHXKAhNPNYJOeGlPNuWpd3Hv6QeTMWEFzni3La+Jl8+dY0ho5o2XLqZgV1EbkfOAw7mfkncJqqrk6ALkMTmP7hD9wx4iFC/hCWpfw0ZTZfvTONW9+7us43/opFq3B7XTFBHcDldbP8nxUtEtTX0NwPovgOQv0f1qqk8diLynIuB/+naOgXxLUVZByClt4M/k9Zm65xg6s3uLauOVrLbo3uE/0ARv5Ai0+Ejl8jDUyOtiX2HrE72+7Rj+njf8DlcTF4+E7kFKSA51DlS1GzM//aMvTqd8AzGJJQ7tpYJr8xNSagA/grA0x+4+sWD+rNtQmYCGyjqtsCvwPXNV+SoSmoKo9d8ByBqiBWtPzRXxlg1ue/Mm/GgjqP6dmvG5FQ/Gg3HAjRu3+Kj0w9gyHjaMBr17ZLJri2QrIvQsSDZAzDkXsDknl09OngDsg8OWr0lAUZhyLtXq75crFTcx8Qm7YBCNsLoTYiCjdpx6Hn7MeBpw1NiYAORD1c/LHbtBqteBJr1XCslQdiVTyT9A5HtcnKjW8qLSJk12Pal0iaFdRV9TPVmiWG04G0mhLX0C9YpddhlVyE+iekZZehoD9I0X8lcdstS/njh7/qPCYzJ4PT7z4Bb6YXEbuqxptpT35l5bX8m645iAiO3BuRwo+Q3NuQgpdsNz5H3bpFPDhyr8TR6XscnWbhyLu3Vv1yhLqNn6xodU1688tXczhr28vZ33Usx3c/h0mvfZlsSU1D6kn5hOfY/0X+goon0ZL1L4BrTY6+Yhi+rFjtngw3h1/Q8hbaicypnw68Wd+LInI2cDZAjx7JHw1a1eOhdCT2I7eFBr6GjC+QvLuTLa1JeHwesvMzKSuKDUBOp4NufeqfUDry4kPYauct+OylLwBlv5P3Yuvd+rSs2AQirh4JyXeLZKDubaJ2uesEd7XAu0ezz59MlvyxlOsPvotAlf0UsurfYh45dxTZ+dnscuiOSVbXOCTrZLT0VuIXg637pBmA4Aw09Mf61yC0EkOOHMTyhSt59Za3CAXDZOb4OP+x09lq55bXt97qFxGZBNS15G2kqr4f3WckMBA4Uhsx3G3t6pfaqEbQFbvYFp0xeJHC9xHXpknRtaF89NxEnr7s5ZoPr8fnpufW3Xniu7ubNdO+saDhhWjxCfYEqyoQgdwbcGQelxQ9s7+Zx4SXpoCl7HfKXmy7R78NOs9Tl43mgyc/JRKOfRLpu8sWPPbtXYmQ2uKoqu3CWDkaxBn9+4SJcwyVbCTvXsS3XzJk1kskHKG8pIKcdtn1Fi40hYRUv6hqg2vpReRU4FBgn8YE9JTAKgL1x28Xl+3Kl2ZB/ZCz9qNd5wLevO99yorK2eOYXTnuqmEtEtBLVpTy3qMfMfubeWy546YcddlhLTqx2hqIqyd0+AICU23DLc9uiDM5hlXjHv2QF0eOqZlkm/Lmt5x441GMuOaIJp9r1b/FcQEdoGR57cFM6iIiSM7laNY5YK1EHR1g5ZD45jcaBPeGffm1JE6Xk/wOea16zeZWvxwIXA3sqVrXWvwUxZEf/davtV0tu9dhGrLrYQPZ9bAGv8CbTcmKUs7e9goqV1cSCoaZO+13Jrz0Bc/8eD+denZo0Wu3NCJu8A1Nqoaq8mpeuH5MTNVEoCrAq7e+zSFn79fkycvBh+/M95/MirGZcHlc7DqsZd8nLYE4ssCRhQBWzvVQdhs1HvaSCRlHIc6uSVaZGjR3KPcEkANMFJGfROSZBGhqcUQ8kHWu7Q5Yg9e26nRtkzRdqc57j35EZakd0ME2d6our+b1O99NsrK2waK5S3C54x/R3V43/8xe3OTz7XXcbmwzeCt82T6cLgcZOT469ijkxBuPToTcpOHIPBpp9wpkDAfvQUjeQ0jODcmWlTI0a6SuqnWbc6QBknUOOLuhlS/audSMQ5Gss1J6QUOy+e3b+YQCsba5kbDFvOmpZe6UrnTsURj3+wUIBUJ06tX0JyGny8ldn4zk5y9+Y96MBXTdvDO7DhvYIlYQrY14tkM82yVbRkqS/n/dDURE7ECecWiypaQNWw7cjDnT5hMOrq08cDgdbL5D6y4Zb6u061zA0BGD+fLtb2PtGw7faYObUogI2w3dhu2GmifQ1kBVKVpagjfDk7Raf1MaYWg0R116CL4s+1Ee7IDuzfRwwsijkqys7XD58+dy0k3H0L5LAfkd8zjikoO59pWLki3L0Aj+nr2IM7a+jJM3v5DjupzFyEPvorK0LsvplsUE9UagoTlYq6/CKjoZq/K1lFu91loUdm3Ps7Pu58DT92bTbXuyz/8N4amZ99Ftiy71HlNZWsm8GX9QuqqsFZW2DsXLSpgzbT4VqxP3wV329wreeehDqiv8+Cv9jHvkI8Y9+lHCzm9oGYKBEFfufQuL5/1LyB8iFAwza9Kv3HPyE62uZaNNvzQWDXyNllzAmkVKhH6yvUfavWF3A9rI6NijA5c+c06j9n319rcZe/d7uL1ugoEQB5+5Dxc8enraz1tEIhEePvtZJr8xFbfXRTgY5sSbjuGQs/blq3em46/0s8thAxv8squPB057itJVZTEdr166cSy7HzGILpt2SuRtGBLIrEm/EA7GzoeEgmFmTviJyrIqsnJbz+Vz44tKTUTLbsP2nlhT7+u3my4Ev02iqtTn+09n8dZ97xP0h6gsrSLkDzFh9BQmvPRFsqU1m/FPf8YXb35LKBCiqqyaoD/Ea7e9zYju5/DMFS/z4sg3OGfAlbz3+MdNOm8kHOG3afPjWxiK8P2nPyXuBtooqiGsihewVh6CtWo4WjWu1aw//FXB+BLpKLWDfUtjgvr6iCyK36ZBCM1tfS1pxEfPTYqpjwbbaGz80xOSpChxfDRqYs3q3TUE/SFC/hCBqgChQJigP8jz17xGyfLVjT6vOAS31x233elykJ3fdv3cE4WuvhQqHoXIHxCeg5bdipY/0CrX3nG/beM6R4kIPft1I6+wdXulmqC+Phx1PEKLt95WawabuhpmA2lpmrahON0ufvlyTqP3dzgcHHzmPngzPLHncTnZ9fCdEi2vTaHhv6OdtdZdKV4NVa+grWDMlp2fxbWvXYw300NmbgaZORkUdM7nhiQ05zZBfX3kXAes66ntBWe3tDd7amkOOmOfOJc6b6aXQ85q0HUiLTj4rH3xZnrWvyNKfsemLRE/+/6T2P+0oXh8bpwuB5tt14uHvryNjKyNw9d9gwn/YzdKqY24wFreKhJ2P2IQb/73HFe/dCE3v3slbyx8eoPmVZrLRtHOrrlocAZa8azdRce7L5J1er1WrwYbVWX0DWN456EP8fjsidJ9/28Ilz57Tr2eNKpKVXk1Gdm+FjciC/qDVFf4yW2f0+SJ25qJ0jFT8XjdhAIhevTtxuJ5/9a0hHO6HHTq2YHR8x/boHsJh8KEgmETzBuJRpahK/clrvm4ZCIdv0Pqs/CNUryshL9/XUTXLbrQuVfHBvdNJo0x9DJB3dCilK4qY9Hcf9lk886071JQ735fvvUtT17yImXFFWRk+zjr3hM5+MzEj+oj4QhPX/4Snzz/OZaldOjWjqtfvohtBm/V5HMVLS1h2d8r6NmvG95MD89f+zofPzeJUCDMTgdux6XPntPgPRsSi1V2J1S9hW3TK4APcq7GkfV/9R6jqoy6+lXef+JTPD77C3rw8J255tWLEuKqmGhMUE8xVKPd7UNz7Jy8e8e0L+9LBHOm/87V+95as4oS7FTNTW9fwc4Hbd/gsQvnLOazl78kFAwxdMTu9B3UsF/16BvH8u7D42Ou5cvy8cqfT1DQxFSJIbVQVQh8gVa/B+JBMkcgnobNy6aNn8ldJzwSM6nvzfRy5j3/x/ALD2ppyU2mMUHd5NRbCdUwuvpctOgktOwutORMtPgEtC4L4I2M/z3+SVw/x0BVgLcf/KDB4yaPncoFO13LOw+N53+Pf8JV+9zC2Hvfa/CYD576NCagA4SCId55cPyGiTekDCKC+IbiKHgMR/4D6w3oABNGT4mr0gpUBfj0hc9bSmaLY4J6a1H9PgSmYz8a+m0TsdBvaOXLyVaWdMqKyqnrgbG8uP6qhVAwxGPnPUegOogVsVBLCVQFefXWt1m9sn6/8NpfHgCRUIR3H/mQh856ZqOqzjFQpysm2JVL6YoJ6q2E+j8kviWXH/xNW6DSFtnruMF19HP0sNdx9Xdd/+/P5VhWfAMIt9fF7zPr7s0KsPNBO+B0xX+QI6EIU8ZO5ecvfmuC8vRDtbpVSvzShUPO2Q9vZnyVVmv0Em0pTFBvLRwF2JM3tbebPO5+J+3BgKHb4M304s304MvysuWOm3LkJQfXe0y7zvlEQpG47eFQhE4963c0vOjJM+p9PVAV4LuPf2z6DaQBalXYDdaXD0RX7IxVdDQarmNh3UbG9nv359TbjsOb6SUzJwOPz81h5+3PfifvmWxpG0z6PmOkGZJ5Cur/nNjRegaSdUayJKUMTpeTOz64lvnfL+DPn/6hR9+ubD14qwYnkXMKstnn/4YweczUmhy5x+em76At6dmve73HtetcwIvzHuX4bufEtXVze93kd2jd1X+thZZeBYGvqentGZqNFp8IHaYgUncKQq0K0ApwdGrTE/pHX34Yh5yzH0v/XE7HHoVk56d3ubIZqbcS4hkAefeAoxBwgeRD7kjEm74jgkTTZ6fNOfisfdlm976NCiKXPH02J4w8ig7d21PQOZ9hFxzI7eOvXe9xTqeTE286Ji7l43Q52fektvf3UKssGtDXnU+w7H6swRnx+2sIq/R6dMUu6Mr90JV7oIHvWk1vMsjI8rHptj3TPqCDKWlsdVQVtNJeFLERujymCqrKWw98wJi7x1FVWkWvbXpw+XPnstXODZdEpiMaWR5dmBNb5YFkI3n3IL79YzZbZfdB1WvELLmXDKTw86Q15DbYmDp1g2E9qCqWZaXkQpNEoaroqgMg8k+tV3xIx28QR07MVmv5jvYoPgYvknMVknVyS0o1rAdTp24wrAcRadMBHaL12/mPguSBZAGZgA/y7okL6IDtQhpHBLR29ZYhFTETpQbDRoC4+0LHr20nQ/WDdwjiqMfCwDsUApOAdX3AXeDbpzWkGpqJCeoGw0aCiA9q5c/r3C/vFrToD7CWAg7QEORchhi76bQgIUFdRK4AHgA6qOqqRJzTYKNWhd2ow9kDcSSnO7lh40Ic7aDwYwj9CNZKcA80E6RpRLODuoh0B/YHzEqGBKKqaMXjUPmc7QmtYTTrTCT74jZdM2xIDUQEPDsmW0arMPW973j5pjcpWlpC/yF9OffBU9K6H2wiJkofBq6m3g59hg0iMAkqXwACdgkkAah6EQKfJVsZAGoVY5U/hlV8Olb5Y6hVnGxJBkOT+Xrcd9xz0mP889tiyosrmDZ+JhcOupbykvS1UmhWUBeRw4F/VfXnRux7tojMFJGZK1eubM5lNwq0agxxXjFajVa90fhzaDXq/xStfg+NFCVOW2QluvJgqBwFwalQOQpddQgaMZm3lkQDX2CtOgJrxa5YJRej4cXJlpT2jL5hTIxrp1pKoDrIpNe+SqKq5rHe9IuITAI61/HSSOB67NTLelHVUcAosOvUm6Bx40Tr6UBe3/bau4XmoMUnY5eiKRBB8+7CkXFY86VVPh+tY44uOScIVhla+TySu/4VnYamo/4p6OpLqFkQFPgMDU6HDhMR4x+0wRT9VxK3LVAVZNlfrdMCryVY70hdVfdV1W1q/wf8BfQGfhaRf4BuwI8iUtcXgKGJSOYxQEatjRnR7Q2jqnYA0LJo6qYKCEDp9faS8eYS/IG1AX0Noeh2Q0ugFQ8R21TZAvXbDSEMG8zWu/WJm6PyZfsYsNc2SVLUfDY4/aKqv6pqR1Xtpaq9gCXADqq6LGHqNmZ8h0LGcMADkmP/33cY+Iat/1hrBUTq+DOIG4LTm6/N3QeovWDHCe6mt4QzNJLI0jo2+u2Gy4YN5ryHTyErLxNPht1I3Jfto++gzRl06A5JVrbhmDr1FEVEkLxb0ezzIPwnuDZDnI18CJIM6p631ugXRDO1ZZ2D+j+OrjC0AAeID8k6q9HnUKsC/B+hkf8Qz07g2c144TSEe3sIfkXM31UyEe+gpElKJ1SVL978lk9e+Byny8lh5+3PbsN2onufrrz0+2NMeOkLlv29nO332ZbdDh+Y1quMjfdLG8UquQACX7LWmc8Bjo5IA1arTUHDf6EVT0BoNri3QbIvRFybNvLYJWjR0dEvhWqQTPAMQvKfNoG9HjT8F1p0THQJfwDIBHdfpN2riJix2fp47MLnmfjyFzWt63xZXo69+nBOunH96cxUwhh6bcSoVYmWjQT/RMACd38k70HEVb/XeGthf+F8butag2QieQ8jvqFJ05XqaKQIrX4HIv8gnsHgOwARd+w+GoTg94AFnp0R8dZ9so2IVf8WcfLmFxEKxM4DeXwe3l7+PJk5GfUcmXo0Jqibr/g2ijiykPxHUA2AhhFHCvlEB78jJqADaBUamGqCegOIsz2SfU69r9sVT6exdhJboGAU0kYWES2a9y+T3/gay1KGjhhM7216NO64uf/i8bnjgrrL42TpX8vZbECvFlCbPExQb+OIeCHVRmuO9hCpXYXjBWeXhF5Gw4vRioftkauzO5JzmZ2/b4OoWmjJuaCxJXpaci50/DZuRJ9uTHr9Kx45+1nCoTCqMO7hDzn34VM59Oz91nts9626xgV0gHAwTOfeHVtCblIxCUxD65N9EeBbZ4OAuJGMIxJ2CbWK0aIj7cbe1nIIzUSLz0Dr6PSTkOupHw18jQa+Qxu5liChhP+0S1jjiEBovWsDU5qgP8hj5z1HoDpIJGxhRSwC1UGeuewlKsuq1nt8h27t2ffEPWI6XfmyvBx9xWFk5Wa2pPSkYEbqhlbHkXEoloah8jGIrALPACTnJsTZvtHnUA2iFU9B9VugFmQMs0fiYudHteot22I2Js3jR8sfRtqPSej9aGAauvp87Mbiak/8tnu5dV0NxWP/HuKwUu9JrYn889tixBHvd+R0O1kw628G7Ln1es9xyTNns/XgrWqqX4adfwBDjtqlJeQmHRPUDUnBkTkcModv8PFaei34J1GzIKdqDBr+A2k32v45/Cdx7dvAdrxMIKp+O6Br5Tobq9CSC6Dw01YzXxNXT9TVG8K/A5HoVgc42oErfRfSALTrUkA4GP/0Ew5F6NCtcQMBh8PB/qfsxf6n7JVgdamHSb8Y0g6NFIH/M2JXWAYg+AMa/hsA8QyK1uuviwPcCV5UEvwee4QeoxAi/4H1X2KvtR6k4FlwbQ14AC+4tkAKXkp7V8/CTdox6NAd8fjWzgu4fW623aMvm2xmFrDXxgR1Q/phrbBXx9ZGXGtXXmYcBs6erLVa8NiNlnOuTLAYD/Uu9MKT4Gs1jDg74yh8B+kwGekwEUfheMTVuAqRVOfaVy/miIsPJr9DLrmFORx27v7cMu6qZMtKSUyduiHtUA2iKwbFpjwA8EYbKedG9wtA9Qdo8FtwbopkjkCcHRKsJYyu3BOsVawN7i5wD8CR4Ny9wWAaTxvaJCIeyLkVu4LGif029kHOVTUB3d7Pi2QegyP/YRw5FyU8oNvXcCHtXgZnL8ALeMA9AMl/IuHXMhgag5koNaQljsxhqKcfWv0BEEF8hyDufknRIq7NofDTaA7djTjbXu2zIX0wQd2QtohrcyTn8mTLAKLt35xdky3DYDDpF4PBYGhLmKBuMBgMbQgT1A0Gg6ENYYK6wWAwtCFMUDcYDIY2hAnqBoMhjhWLVnL3SY8xots5XLzbSH6c9EuyJRkaiQnqBoMhhorVlZw/8Fq+GPsNRf8VM3f679w0/F6+/3RWsqUZGoEJ6gaDIYbPXv4Cf6UfK7LWyjdQFeTFkcb2IB0wQd1gMMSweP6/BKqDcduX/bMiCWoMTaXZQV1ELhKReSLym4jclwhRBoMheWy7x9YxXYIARKDfrlsmSZGhKTQrqIvIUOBwYICqbg08kBBVBoMhaQw5ahC9tu5eE9jdXjcZORmcff/JSVZmaAzN9X45D7hHVQMAqmqezwyGFGXlkiJG3zCGn7/4jY49O3DKLcey3dD4rkgut4uHvrqNKWO+4YeJP9N1iy4ccvZ+tO9SkATVhqbSLD91EfkJeB84ELsNzZWq+n09+54NnA3Qo0ePHRcuXLjB1zUYDE2jYnUlp/W5mLLiipoJUG+mh1vfu5od9xuQZHWGxpIQP3URmSQis+v473DskX47YBfgKuAtqad3lqqOUtWBqjqwQ4fE+1obDOnK/O8XcPFu13No9v9xer9LmTY+8Q1kJr7yBdV1VLS8cN3rCb+WIbmsN/2iqvvW95qInAeMU3u4P0NELKAQWJk4iQZD22XpX8u5cu9b8FfaTbIXz/uXO49/mNvev5Yd9umfsOssmvsvgaq6KlrMR7Wt0dzql/8BQwFEZEvspoyrmnlOg2Gj4f0nPyEUDMdsC1QFee32txN6nf5D+uLL9sVsE4Gtdt4iodcxJJ/mBvUXgU1FZDYwFjhFk9H01GBIU5b+vYJIKBK3fdWS4oReZ8jRu9C9zyZ4M6MVLR4XGTkZnPPASQm9jiH5NKv6RVWDwIkJ0mIwbHTscsiO/Djxl5r0C4DL7WSnA7dL6HXcHjePfH07k9+YyszPfqbbFp059Nz9KezaPqHXMSSfZlW/bCgDBw7UmTMTPxlkMKQbwUCIq/a+hb9/XUR1hR9flpecdtk8+f29FHTMS7Y8Q4rRmOoX06PUYEgiHq+bh768jekf/sC8GX/QvU9X9jhmV3yZ3vUfbDDUgQnqBkOScbqcDB6+M4OH75xsKYY2gDH0MhiilBWVM238TOZ+9wdmvt+QrpiRusEAfPjsZzx12Uu4PS4sS+nSuyP3f34zeYW5yZZmMDQJM1I3bPT8u2ApT1/+MiF/iKqyavwVfhbN+5fHLng+2dIMhiZjgrpho+fb/32PrrN8HiASijDtgzptjAyGlMYEdcNGj9vnRpzxHwWX22QnDemHCeqGjZ49j9kVRy0fOk+GmwNO3Ss5ggyGZmCCumGjp6BTPre+fw0FnfLwZnpxe90MHj6Is+4zS+gN6Yd5vjQYgB326c/Yf0ex/J+VZBdkkVOQnWxJBsMGYYK6wRDF4XDQZdNOyZZhMDQLk34xGAyGNoQJ6gaDwdCGMEHdYDAY2hAmqBsMBkMbwgR1g8FgaEMkpUmGiKwEFtbzciFts89pW70vMPeWjrTV+4K2e2+FQJaqdmhop6QE9YYQkZnr6+yRjrTV+wJzb+lIW70vaLv31tj7MukXg8FgaEOYoG4wGAxtiFQM6qOSLaCFaKv3Bebe0pG2el/Qdu+tUfeVcjl1g8FgMGw4qThSNxgMBsMGYoK6wWAwtCFSNqiLyEUiMk9EfhOR+5KtJ5GIyBUioiJSmGwtiUJE7o/+vX4RkfdEJD/ZmpqDiBwoIvNFZIGIXJtsPYlCRLqLyBQRmRP9bF2SbE2JREScIjJLRD5MtpZEIiL5IvJO9DM2V0R2rW/flAzqIjIUOBwYoKpbAw8kWVLCEJHuwP7AomRrSTATgW1UdVvgd+C6JOvZYETECTwJHAT0A44XkX7JVZUwwsAVqtoP2AW4oA3dG8AlwNxki2gBHgU+VdWtgAE0cI8pGdSB84B7VDUAoKorkqwnkTwMXA20qRlqVf1MVcPRH6cD3ZKpp5nsDCxQ1b9UNQiMxR5kpD2qulRVf4z+uxw7OHRNrqrEICLdgEOA55OtJZGISB6wB/ACgKoGVXV1ffunalDfEhgiIt+JyJcislOyBSUCETkc+FdVf062lhbmdOCTZItoBl2Bxev8vIQ2EvjWRUR6AdsD3yVZSqJ4BHvAZCVZR6LpDawERkdTS8+LSFZ9Oyet85GITAI61/HSSGxd7bAfD3cC3hKRTTUN6i/Xc1/XY6de0pKG7k1V34/uMxL7Ef/11tRmaBoikg28C1yqqmXJ1tNcRORQYIWq/iAieyVZTqJxATsAF6nqdyLyKHAtcGN9OycFVd23vtdE5DxgXDSIzxARC9vMZmVr6dtQ6rsvEemP/Y37s9id67sBP4rIzqq6rBUlbjAN/c0ARORU4FBgn3T4Am6Af4Hu6/zcLbqtTSAibuyA/rqqjku2ngQxGBgmIgcDPiBXRF5T1ROTrCsRLAGWqOqaJ6p3sIN6naRq+uV/wFAAEdkS8JDmrmuq+quqdlTVXqraC/sPtUO6BPT1ISIHYj/6DlPVqmTraSbfA1uISG8R8QAjgA+SrCkhiD2ieAGYq6oPJVtPolDV61S1W/SzNQKY3EYCOtEYsVhE+kQ37QPMqW//VG08/SLwoojMBoLAKWk+8tsYeALwAhOjTyLTVfXc5EraMFQ1LCIXAhMAJ/Ciqv6WZFmJYjBwEvCriPwU3Xa9qn6cPEmGRnAR8Hp0kPEXcFp9OxqbAIPBYGhDpGr6xWAwGAwbgAnqBoPB0IYwQd1gMBjaECaoGwwGQxvCBHWDwWBoQ5igbjAYDG0IE9QNBoOhDfH/HIVTYi8towwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xTrSpiral[:, 0], xTrSpiral[:, 1], s=30, c=yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3183e35d16b85a5f474e39a15cd14f5e",
     "grade": false,
     "grade_id": "cell-d73f1d288cf74deb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code loads the ION dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8223c9331576477c95d446780580ff1",
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in ION dataset: 281\n",
      "Number of testing points in ION dataset: 70\n",
      "Number of features in ION dataset: 34\n",
      "Training set: (n x d matrix)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$y$</th>\n",
       "      <th>$[\\mathbf{x}]_{1}$</th>\n",
       "      <th>$[\\mathbf{x}]_{2}$</th>\n",
       "      <th>$[\\mathbf{x}]_{3}$</th>\n",
       "      <th>$[\\mathbf{x}]_{4}$</th>\n",
       "      <th>$[\\mathbf{x}]_{5}$</th>\n",
       "      <th>$[\\mathbf{x}]_{6}$</th>\n",
       "      <th>$[\\mathbf{x}]_{7}$</th>\n",
       "      <th>$[\\mathbf{x}]_{8}$</th>\n",
       "      <th>$[\\mathbf{x}]_{9}$</th>\n",
       "      <th>...</th>\n",
       "      <th>$[\\mathbf{x}]_{25}$</th>\n",
       "      <th>$[\\mathbf{x}]_{26}$</th>\n",
       "      <th>$[\\mathbf{x}]_{27}$</th>\n",
       "      <th>$[\\mathbf{x}]_{28}$</th>\n",
       "      <th>$[\\mathbf{x}]_{29}$</th>\n",
       "      <th>$[\\mathbf{x}]_{30}$</th>\n",
       "      <th>$[\\mathbf{x}]_{31}$</th>\n",
       "      <th>$[\\mathbf{x}]_{32}$</th>\n",
       "      <th>$[\\mathbf{x}]_{33}$</th>\n",
       "      <th>$[\\mathbf{x}]_{34}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     $y$  $[\\mathbf{x}]_{1}$  $[\\mathbf{x}]_{2}$  $[\\mathbf{x}]_{3}$  \\\n",
       "0   -1.0                0.55                1.66               -0.56   \n",
       "1   -1.0                0.23               -0.04                1.61   \n",
       "2   -1.0                2.18               -1.05                0.27   \n",
       "3   -1.0               -2.10                1.56                0.55   \n",
       "4   -1.0                1.55                0.18                0.74   \n",
       "..   ...                 ...                 ...                 ...   \n",
       "276  1.0                3.37               -1.57               -1.80   \n",
       "277  1.0               -0.05                0.03                0.39   \n",
       "278  1.0               -1.34               -0.13               -0.36   \n",
       "279  1.0               -1.24                0.13               -0.09   \n",
       "280  1.0               -2.00               -0.19               -0.29   \n",
       "\n",
       "     $[\\mathbf{x}]_{4}$  $[\\mathbf{x}]_{5}$  $[\\mathbf{x}]_{6}$  \\\n",
       "0                  1.01                1.83               -0.46   \n",
       "1                 -0.09               -0.23               -1.59   \n",
       "2                  2.20                2.15               -2.48   \n",
       "3                 -1.53               -0.92               -0.31   \n",
       "4                  0.78                0.18                0.38   \n",
       "..                  ...                 ...                 ...   \n",
       "276               -2.17                0.13                0.04   \n",
       "277                0.11               -0.05                0.09   \n",
       "278                0.46                1.19                0.07   \n",
       "279               -0.27                0.06               -0.03   \n",
       "280               -0.09               -0.04               -0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{7}$  $[\\mathbf{x}]_{8}$  $[\\mathbf{x}]_{9}$  ...  \\\n",
       "0                  1.91               -1.48               -1.15  ...   \n",
       "1                  1.35                0.34               -0.03  ...   \n",
       "2                 -0.13                1.76                0.14  ...   \n",
       "3                  2.06               -0.30               -0.15  ...   \n",
       "4                 -0.10               -0.21               -0.63  ...   \n",
       "..                  ...                 ...                 ...  ...   \n",
       "276                0.32                0.10               -0.40  ...   \n",
       "277               -0.10                0.17                0.16  ...   \n",
       "278                0.34               -0.38                0.23  ...   \n",
       "279               -0.32                0.09                0.17  ...   \n",
       "280                0.07                0.08               -0.07  ...   \n",
       "\n",
       "     $[\\mathbf{x}]_{25}$  $[\\mathbf{x}]_{26}$  $[\\mathbf{x}]_{27}$  \\\n",
       "0                  -0.51                -0.46                 0.58   \n",
       "1                   0.36                -0.24                 0.06   \n",
       "2                   0.02                 0.27                 0.08   \n",
       "3                  -0.53                 0.30                -0.74   \n",
       "4                   0.01                 0.19                -0.18   \n",
       "..                   ...                  ...                  ...   \n",
       "276                 0.29                 0.14                 0.06   \n",
       "277                -0.09                -0.06                 0.07   \n",
       "278                -0.24                 0.07                 0.18   \n",
       "279                 0.09                -0.06                -0.02   \n",
       "280                 0.00                -0.03                 0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{28}$  $[\\mathbf{x}]_{29}$  $[\\mathbf{x}]_{30}$  \\\n",
       "0                   0.30                 0.19                -0.30   \n",
       "1                  -0.21                -0.21                -0.04   \n",
       "2                   0.15                -0.22                 0.09   \n",
       "3                   0.03                -0.14                 0.23   \n",
       "4                  -0.18                 0.02                -0.04   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.47                 0.30                 0.07   \n",
       "277                -0.15                -0.04                -0.09   \n",
       "278                 0.06                 0.00                -0.66   \n",
       "279                 0.07                 0.04                -0.01   \n",
       "280                -0.02                -0.01                -0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{31}$  $[\\mathbf{x}]_{32}$  $[\\mathbf{x}]_{33}$  \\\n",
       "0                  -0.48                -0.45                -0.09   \n",
       "1                   0.00                 0.22                 0.08   \n",
       "2                  -0.34                -0.45                -0.15   \n",
       "3                   0.00                -0.18                -0.02   \n",
       "4                  -0.22                 0.19                 0.05   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.54                 0.27                -0.12   \n",
       "277                 0.12                -0.15                -0.09   \n",
       "278                 0.11                 0.19                 0.13   \n",
       "279                -0.03                -0.05                -0.04   \n",
       "280                -0.01                -0.00                 0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{34}$  \n",
       "0                   -0.0  \n",
       "1                   -0.0  \n",
       "2                   -0.0  \n",
       "3                    0.0  \n",
       "4                   -0.0  \n",
       "..                   ...  \n",
       "276                  0.0  \n",
       "277                 -0.0  \n",
       "278                 -0.0  \n",
       "279                 -0.0  \n",
       "280                  0.0  \n",
       "\n",
       "[281 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "print(f'Number of training points in ION dataset: {xTrIon.shape[0]}')\n",
    "print(f'Number of testing points in ION dataset: {xTeIon.shape[0]}')\n",
    "print(f'Number of features in ION dataset: {xTrIon.shape[1]}')\n",
    "print('Training set: (n x d matrix)')\n",
    "TrIon_for_display = np.concatenate([yTrIon[:, None], xTrIon], axis=1)\n",
    "TrIon_for_display = TrIon_for_display[TrIon_for_display[:, 0].argsort()]\n",
    "\n",
    "display(pd.DataFrame(data=TrIon_for_display,\n",
    "                     columns=['$y$'] + [f'$[\\mathbf{{x}}]_{ {i+1} }$' for i in range(xTrIon.shape[1])]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1471471f16e7153e9f7916d8893fab91",
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part One: Implement `sqimpurity` [Graded]\n",
    "\n",
    "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
     "grade": false,
     "grade_id": "cell-ec2301f1325f79b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity(yTr):\n",
    "    \"\"\"\n",
    "    Computes the squared loss impurity (variance) of the labels.\n",
    "    \n",
    "    Input:\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    N, = yTr.shape\n",
    "    assert N > 0 # must have at least one sample\n",
    "    impurity = 0\n",
    "    \n",
    "    mean = np.mean(yTr, axis = 0)\n",
    "    square_diff = (yTr - mean)**2\n",
    "    impurity = np.sum(square_diff, axis = 0)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e61f2dc5aeb81315a4e40b9306e92d",
     "grade": false,
     "grade_id": "cell-ba20a92528e16fbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: sqimpurity_test1 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test2 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test3 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test4 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def sqimpurity_test1():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isscalar(impurity)  # impurity should be scalar\n",
    "\n",
    "def sqimpurity_test2():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return impurity >= 0 # impurity should be nonnegative\n",
    "\n",
    "def sqimpurity_test3():\n",
    "    yTr = np.ones(100) # generate an all one vector as labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isclose(impurity, 0) # impurity should be zero since the labels are homogeneous\n",
    "\n",
    "def sqimpurity_test4():\n",
    "    yTr = np.arange(-5, 6) # generate a vector with mean zero\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    sum_of_squares = np.sum(yTr ** 2) \n",
    "    return np.isclose(impurity, sum_of_squares) # with mean zero, then the impurity should be the sum of squares\n",
    "\n",
    "def sqimpurity_test5():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr)\n",
    "    impurity_grader = sqimpurity_grader(yTr)\n",
    "    return np.isclose(impurity, impurity_grader)\n",
    "\n",
    "runtest(sqimpurity_test1, 'sqimpurity_test1')\n",
    "runtest(sqimpurity_test2, 'sqimpurity_test2')\n",
    "runtest(sqimpurity_test3, 'sqimpurity_test3')\n",
    "runtest(sqimpurity_test4, 'sqimpurity_test4')\n",
    "runtest(sqimpurity_test5, 'sqimpurity_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e4b6cdeffb9e6abb0e20f1426d5e335",
     "grade": true,
     "grade_id": "cell-84a2790fead7b76f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "295c7921f17373bcee865218851434f0",
     "grade": true,
     "grade_id": "cell-835351931df18818",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15be06d818d669ee3d72cd31f9849813",
     "grade": true,
     "grade_id": "cell-3c3b7a31a818b505",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "465b390daf35d5c677b377e4a98633ee",
     "grade": true,
     "grade_id": "cell-1c7dc9e1879e6a32",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0906343847ad8e1e8800b1c85bd474a7",
     "grade": true,
     "grade_id": "cell-330bca5d42fef37a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "110d11c6a9b86f38e65eb9d517778dac",
     "grade": false,
     "grade_id": "cell-c8238fba4c3de7ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraction_pos</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraction_pos  impurity\n",
       "0            1.0       0.0\n",
       "1            0.9       3.6\n",
       "2            0.8       6.4\n",
       "3            0.7       8.4\n",
       "4            0.6       9.6\n",
       "5            0.5      10.0\n",
       "6            0.4       9.6\n",
       "7            0.3       8.4\n",
       "8            0.2       6.4\n",
       "9            0.1       3.6\n",
       "10           0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Squared loss impurity of labels')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 10\n",
    "y = np.ones(size)\n",
    "fraction_pos, impurities = [], []\n",
    "for i in range(size):\n",
    "    fraction_pos.append(sum([y == 1]) / size)\n",
    "    impurities.append(sqimpurity(y))\n",
    "    y[i] = -1\n",
    "fraction_pos.append(sum([y == 1]) / size)\n",
    "impurities.append(sqimpurity(y))\n",
    "\n",
    "display(pd.DataFrame(data={'fraction_pos': fraction_pos, 'impurity': impurities}))\n",
    "plt.plot(fraction_pos, impurities)\n",
    "plt.grid()\n",
    "plt.xlabel('Fraction of positive labels (1 - fraction of negative labels)')\n",
    "plt.ylabel('Squared loss impurity of labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
     "grade": false,
     "grade_id": "cell-4730d6e94ace8e06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Implement `sqsplit` [Graded]\n",
    "\n",
    "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
    "\n",
    "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
    "\n",
    "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
    "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
    "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
    "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
    "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
    "\n",
    "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2\\\\\n",
    "2 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
    "\n",
    "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
    "    \n",
    "    Input:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: squared loss impurity of the best cut\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    assert d > 0 # must have at least one dimension\n",
    "    assert n > 1 # must have at least two samples\n",
    "    \n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    #sorting xTr along each feature\n",
    "    xTr_sort = np.sort(xTr, axis = 0)\n",
    "    \n",
    "    for f in range(d):        \n",
    "        for i in range(n-1):\n",
    "            if xTr_sort[i, f] == xTr_sort[i+1, f]:\n",
    "                continue\n",
    "            cut_val = (xTr_sort[i, f] + xTr_sort[i+1, f])/2\n",
    "            left = xTr[:, f] <= cut_val\n",
    "            right = xTr[:, f] > cut_val\n",
    "            \n",
    "            lbl_left = yTr[left]\n",
    "            lbl_right = yTr[right]\n",
    "            \n",
    "            impurity_left = sqimpurity(lbl_left)\n",
    "            impurity_right = sqimpurity(lbl_right)\n",
    "            \n",
    "            loss = impurity_left + impurity_right\n",
    "            if loss < bestloss:\n",
    "                bestloss = loss\n",
    "                feature = f\n",
    "                cut = cut_val\n",
    "    return feature, cut, bestloss\n",
    "            \n",
    "            \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    #return feature, cut, bestloss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbc3579da8f3891cc0b907caf6f397f",
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.81 seconds\n",
      "The best split is on feature 2 on value 0.304\n",
      "Your tree split on feature 2 on value: 0.304 \n",
      "\n",
      "Running Test: sqsplit_test1 ... ✔ Passed!\n",
      "Running Test: sqsplit_test2 ... ✔ Passed!\n",
      "Running Test: sqsplit_test3 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4, yor4)[2] / len(yor4), .25)\n",
    "    b = np.isclose(sqsplit(xor3, yor3)[2] / len(yor3), .25)\n",
    "    c = np.isclose(sqsplit(xor2, yor2)[2] / len(yor2), .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    _, cut, _ = sqsplit(x, y)\n",
    "    return cut <= 500 or cut >= 499\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid, cut, loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut >= 0 or cut <= 1) and np.isclose(loss / len(yor5), 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cbffe6477a05dd8d9db574f0d94edcfa",
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b0da843388d7a9ddf59c80d408d3859",
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d84abb3781b6b263676a20c5ed6418",
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d358dfc0f25657b18024284428aec959",
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Implement `cart` [Graded]\n",
    "\n",
    "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
    "\n",
    "**Implementation Notes:**\n",
    "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
    "\n",
    "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
    "1. `left`: node describing left subtree\n",
    "2. `right`: node describing right subtree\n",
    "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
    "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
    "5. `prediction`: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"\n",
    "    Tree class.\n",
    "    \n",
    "    (You don't _need_ to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, feature, cut, prediction):\n",
    "        # Check that all or no arguments are None\n",
    "        node_or_leaf_args = [left, right, feature, cut]\n",
    "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
    "        \n",
    "        # Check that all None <==> leaf <==> prediction not None\n",
    "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
    "        if all([arg == None for arg in node_or_leaf_args]):\n",
    "            assert prediction is not None\n",
    "        if all([arg != None for arg in node_or_leaf_args]):\n",
    "            assert prediction is None\n",
    "        \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.feature = feature \n",
    "        self.cut = cut\n",
    "        self.prediction = prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eac0b2166f8fddde8c51b4ab8470e47",
     "grade": false,
     "grade_id": "cell-5b554dfec9394ba9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell contains some examples of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a tree that predicts everything as zero ==> prediction 0\n",
    "# In this case, it has no left or right children (it is a leaf node) ==> left = None, right = None, feature = None, cut = None\n",
    "root = TreeNode(None, None, None, None, 0)\n",
    "\n",
    "\n",
    "# The following that a tree with depth 2 or a tree with one split \n",
    "\n",
    "# The tree will return a prediction of 1 if an example falls under the left subtree\n",
    "# Otherwise it will return a prediction of 2\n",
    "# To start, first create two leaf node\n",
    "left_leaf = TreeNode(None, None, None, None, 1)\n",
    "right_leaf = TreeNode(None, None, None, None, 2)\n",
    "\n",
    "# Now create the parent or the root\n",
    "# Suppose we split at feature 0 and cut of 1 and the prediction is None\n",
    "root2 = TreeNode(left_leaf, right_leaf, 0, 1, None)\n",
    "\n",
    "# Now root2 is the tree we desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
     "grade": false,
     "grade_id": "cell-d6c63e37fd6c395f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
    "\n",
    "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
    "\n",
    "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de3750e532687607733d83dcc9a0a2dc",
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Builds a CART tree.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "\n",
    "    Output:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    node = None\n",
    "    prediction = np.mean(yTr)\n",
    "    \n",
    "    if np.all(np.isclose(xTr, xTr[0])):\n",
    "        return TreeNode(None, None, None, None, prediction)\n",
    "\n",
    "    if np.all(np.isclose(yTr, yTr[0])):\n",
    "        return TreeNode(None, None, None, None, prediction)\n",
    "\n",
    "    feature, cut, _ = sqsplit(xTr, yTr)\n",
    "\n",
    "    left_indices = xTr[:, feature] <= cut\n",
    "    right_indices = xTr[:, feature] > cut\n",
    "    \n",
    "    #When there's no good split\n",
    "    if np.all(left_indices) or np.all(right_indices):\n",
    "        return TreeNode(None, None, None, None, prediction)\n",
    "\n",
    "    left_subtree = cart(xTr[left_indices], yTr[left_indices])\n",
    "    right_subtree = cart(xTr[right_indices], yTr[right_indices])\n",
    "\n",
    "    return TreeNode(left_subtree, right_subtree, feature, cut, None)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc022deb75c9f2be5071b44c001e6d82",
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: cart_test1 ... ✔ Passed!\n",
      "Running Test: cart_test2 ... ✔ Passed!\n",
      "Running Test: cart_test3 ... ✔ Passed!\n",
      "Running Test: cart_test4 ... ✔ Passed!\n",
      "Running Test: cart_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "#test case 4\n",
    "def cart_test4():\n",
    "    X = np.ones((5, 2)) # Create a dataset with identical examples\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # On this dataset, your cart algorithm should return a single leaf\n",
    "    # node with prediction equal to 1\n",
    "    t = cart(X, y)\n",
    "    \n",
    "    # t has no children\n",
    "    children_check = (t.left is None) and (t.right is None) \n",
    "    \n",
    "    # Make sure t does not cut any feature and at any value\n",
    "    feature_check = (t.feature is None) and (t.cut is None)\n",
    "    \n",
    "    # Check t's prediction\n",
    "    prediction_check = np.isclose(t.prediction, 1)\n",
    "    return children_check and feature_check and prediction_check\n",
    "\n",
    "#test case 5\n",
    "def cart_test5():\n",
    "    X = np.arange(4).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1])\n",
    "\n",
    "    t = cart(X, y) # your cart algorithm should generate one split\n",
    "    \n",
    "    # check whether you set t.feature and t.cut to something\n",
    "    return t.feature is not None and t.cut is not None\n",
    "\n",
    "\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')\n",
    "runtest(cart_test4,'cart_test4')\n",
    "runtest(cart_test5,'cart_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b47e3031bb2beb61cae1c347d53b283",
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e322a7a1bab0a83d65ad27517c275cab",
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6bf0051fdf9a7296e893eb66c16b96",
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07b92c1bbd6474aa04712ddb4dbcc0ca",
     "grade": true,
     "grade_id": "cell-cart_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3a55c57681ae193e2771512f553e528",
     "grade": true,
     "grade_id": "cell-cart_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faf306bddae40ade7985dcf6439b861c",
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Four: Implement `evaltree` [Graded]\n",
    "\n",
    "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
    "\n",
    "Here's some inspiration:\n",
    "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
    "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f3f523c3529265fb0ce24f6fea9361",
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(tree, xTe):\n",
    "    \"\"\"\n",
    "    Evaluates testing points in xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        tree: TreeNode decision tree\n",
    "        xTe:  m x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: m-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m, d = xTe.shape\n",
    "    preds = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        temp_tree = tree\n",
    "        \n",
    "        while True:\n",
    "            if temp_tree.left is None or temp_tree.right is None:\n",
    "                preds[i] = temp_tree.prediction\n",
    "                break\n",
    "            elif xTe[i, temp_tree.feature] >= temp_tree.cut:\n",
    "                temp_tree = temp_tree.right\n",
    "            else:\n",
    "                temp_tree = temp_tree.left\n",
    "    return preds\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59551843c4d5ab8b8c0924087bb9202c",
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.05 seconds\n",
      "Training RMSE : 0.00\n",
      "Testing  RMSE : 0.69 \n",
      "\n",
      "Running Test: evaltree_test1 ... ✔ Passed!\n",
      "Running Test: evaltree_test2 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, 0, 10, None)\n",
    "    y = TreeNode(x, c, 0, 20, None)\n",
    "    z = TreeNode(d, e, 0, 40, None)\n",
    "    t = TreeNode(y, z, 0, 30, None)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d772deb41878ca9f83b8c3661ac75b67",
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68d89de2748bcdc384eb209588ffb84d",
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfac59f41d855c0f8b4e1e7bf394c7d",
     "grade": false,
     "grade_id": "cell-030c38de62d6adc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Training and Testing a Classification Tree on the ION Dataset</h3>\n",
    "\n",
    "<p> The following code create a classification tree on the ION dataset and then apply the learned tree to an unknown dataset. If you implement everything correctly, you should get a training RSME that is close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efc7b56e0189a0d123d3ac40b1011afa",
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Visualize Your Tree</h3>\n",
    "\n",
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694a973d8d7c2709f769a08c64cf8f0f",
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=None,b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w is not None:\n",
    "        w = np.array(w).flatten()\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X), xTrSpiral, yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1fe9d13867759f9403c562fe900853a",
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Click to add positive points and use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn also provides an implementation of [Regression Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (and [Decision Tree Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). The usage is pretty straight-forward: define the regression tree with the impurity function (and other settings), fit to the training set, and evaluate on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "t0 = time.time()\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((tree.predict(xTrSpiral) - yTrSpiral)**2)\n",
    "te_err   = np.mean((tree.predict(xTeSpiral) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides a tree plotting function, which is again quite simple to use. This is extremely useful while debugging a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = plot_tree(tree, ax=ax, precision=2, feature_names=[f'$[\\mathbf{{x}}]_{i+1}$' for i in range(2)], filled=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
